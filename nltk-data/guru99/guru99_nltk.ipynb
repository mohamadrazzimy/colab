{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yWF07RI9BPVK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GLDvWjA0BODA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK Tokenize\n",
        "\n",
        "https://www.guru99.com/tokenize-words-sentences-nltk.html"
      ],
      "metadata": {
        "id": "wEpojqM2BgPX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "376HcGc6Agi0",
        "outputId": "00566861-a694-467e-d5bb-72a5abb222ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text = \"God is Great! I passed the exam.\"\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAxQGjctCf8K",
        "outputId": "db142371-ae79-4556-9fe9-16539f4eabd6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['God', 'is', 'Great', '!', 'I', 'passed', 'the', 'exam', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization of Sentences\n",
        "Sub-module available for the above is sent_tokenize. An obvious question in your mind would be why sentence tokenization is needed when we have the option of word tokenization. Imagine you need to count average words per sentence, how you will calculate? For accomplishing such a task, you need both NLTK sentence tokenizer as well as NLTK word tokenizer to calculate the ratio. Such output serves as an important feature for machine training as the answer would be numeric."
      ],
      "metadata": {
        "id": "c8jYSWygC0pM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rSodVD89C25i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}